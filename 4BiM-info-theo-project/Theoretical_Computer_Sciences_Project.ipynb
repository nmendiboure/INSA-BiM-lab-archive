{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Computer Sciences Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sergio Peignier and Théotime Grohens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Libraries to install before starting** :\n",
    "\n",
    "`pip install pqdict`\n",
    "\n",
    "`pip install community`\n",
    "\n",
    "`pip install python-louvain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pqdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import community as community_louvain\n",
    "import math \n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Graph Therory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will apply graph algorithms to study the gene regulatory network (GRN)\n",
    "of *Saccharomyces cerevisiae*. This species of yeast, it is a small single-cell eukaryote, with a short generation time, and two possible forms: an haploid one and a diploid one. Moreover, this organism can be easily cultured, and it has an important economic impact since it is extensively used for instance,\n",
    "in winemaking, baking, and brewing. Due to these characteristics, Saccharomyces cerevisiae\n",
    "is studied as an important model organism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we will study the **gene regulatory network** of *Saccharomyces cerevisiae*, using graph theory algorithms. The files that are provided for this project have been used in [MCK+12] , as gold-standards to assess gene regulatory network inference algorithms, and they are the result of biological experiments based on ChIP binding data [MWG + 06], and systematic transcription factor deletions [HKI07]. Hereafter we describe each dataset in details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRN_edges_S_cerevisiae.txt : contains the edges of the *S. cervisiae* regulatory network (from transcription factors to target genes). The intended meaning is that if there is an edge between transcription factor X and the target gene A, then X regulates the transcription of A ;\n",
    "\n",
    "- net4_transcription_factors.tsv : Is a file containing in a single column the identifiers of the transcription factors of *S. cervisiae* that were studied ;\n",
    "\n",
    "- net4_gene_ids.tsv : The two previous files, use specific identifiers to denote genes, and this file contains the gene name associated to each gene identifier ;\n",
    "\n",
    "- go_slim_mapping.tab.txt : Only columns 0 and 5 will be used in this work. Column 0 contains the gene name, and column 5 contains its Gene Ontology (GO) annotation (http://www.geneontology.org/). Notice that two different rows may give for the same gene different Gene Ontology annotations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Exploration and characterization of the gene regulatory network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 - Load the dataset and create a NetworkX graph instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les datasets avec pandas : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : transcription factors\n",
    "# A : genes\n",
    "GRN_edges_SC = pd.read_csv(\"./datas/GRN_edges_S_cerevisiae.txt\", sep = ',',  header=0) # edges X -> A\n",
    "net4_transcription_factors = pd.read_csv(\"./datas/net4_transcription_factors.tsv\", sep = '\\n',  header=0) # X ID\n",
    "net4_gene_ids = pd.read_csv(\"./datas/net4_gene_ids.tsv\", sep = '\\t', header=0) # A ID -> Name\n",
    "go_slim_mappingtab = pd.read_csv(\"./datas/go_slim_mapping.tab.txt\", sep = '\\t', header=None) # A [0]Name -> [5]GO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRN_edges_SC = GRN_edges_SC.iloc[:,1:] # the first column isn't relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que tout a bien été importé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut transformer le df en array (au cas où si besoin)\n",
    "GRN_edges_SC_np = GRN_edges_SC.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4_gene_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4_transcription_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_slim_mappingtab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRN_edges_SC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset a bien été importé, on créé donc un graphe $G = <V,E>$ dont l'ensemble des noeuds noté $V$ contient les facteurs de transcription et les gènes cibles et l'ensemble des arettes noté $E$ représente les régulations des gènes par les facteurs de transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(GRN_edges_SC, \"transcription_factor\", \"target_gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 - Plot the gene regulatory network,  the plot should be readable,  understandable,  andinformative.  Which information did you decide to convey in your plot?  Why?\n",
    "\n",
    "On représente le graphe G tel quel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Premiere impression pour le graphe \n",
    "plt.figure(figsize=(15,8))\n",
    "nx.draw(G, node_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble $V$ peut être séparé en deux sous-ensembles :\n",
    "\n",
    "- $X$ : ensemble des facteurs de transcription ;\n",
    "- $A$ : ensemble des gènes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gn_nodes = [a for a in GRN_edges_SC[\"target_gene\"]]\n",
    "#tf_nodes = [x for x in GRN_edges_SC[\"transcription_factor\"]]\n",
    "tf_nodes = []\n",
    "for x in GRN_edges_SC[\"transcription_factor\"] :\n",
    "    if x not in tf_nodes :\n",
    "        tf_nodes.append(x)\n",
    "gn_nodes = []\n",
    "for a in GRN_edges_SC[\"target_gene\"] :\n",
    "    if a not in gn_nodes and a not in tf_nodes:\n",
    "        gn_nodes.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos = nx.spring_layout(G, k=0.6) #return the relative positions of the nodes,k = optimal distance between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On rend le graphe plus lisible \n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=gn_nodes, node_color=\"r\", node_size= 18, label = \"gene's nodes\")\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=tf_nodes, node_color=\"b\", node_size= 18, label = \"TGF's nodes\")\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, node_size = 18, pos = pos, width = 0.4, edge_color = 'DarkSlateGray')\n",
    "\n",
    "plt.legend()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If it's a Bipartite Graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networkx.algorithms import bipartite # on a déjà importé tout networkX pas la peinne de réimporter bipartite ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG =  nx.Graph()\n",
    "BG.add_nodes_from(GRN_edges_SC['transcription_factor'], bipartite = 'transcription_factor')\n",
    "BG.add_nodes_from(GRN_edges_SC['target_gene'], bipartite = 'target_gene')\n",
    "BG.add_edges_from(zip(GRN_edges_SC['transcription_factor'], GRN_edges_SC['target_gene']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf = transcription_factor\n",
    "tf_nodes0 = [ n for n in BG.nodes() if BG.nodes[n]\n",
    "            ['bipartite'] == 'transcription_factor']\n",
    "\n",
    "# gn = gene = target_gene\n",
    "gn_nodes0 = [ n for n in BG.nodes() if BG.nodes[n]\n",
    "           ['bipartite'] == 'target_gene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_nodes))\n",
    "print(len(tf_nodes0))\n",
    "len(tf_nodes0)+len(gn_nodes0)==len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posBG = nx.bipartite_layout(BG, tf_nodes0, scale = 1)\n",
    "plt.figure(figsize=(30,20))\n",
    "nx.draw(BG, pos = posBG, node_size = 14,\n",
    "       node_color = 'forestgreen',\n",
    "       edge_color = 'darkblue',\n",
    "       width = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On the graph above we have a sight of our graph *G* if this one was bipartite \n",
    "\n",
    "- On the left side : nodes that represent the transcription factors ;\n",
    "\n",
    "- On the right side : nodes that represent the target genes to apply a regulation.\n",
    "\n",
    "FInaly it is immediately noticeable that certain factors act on a greater number of target genes than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 - Describe the network by computing pertinent local and global metrics,  explain your choices, represent the results graphically if necessary, and interpret the results.\n",
    "##### 1.1.3.1 - Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_degree = {x : len(G[x]) for x in tf_nodes} \n",
    "gn_degree = {a : len(G[a]) for a in gn_nodes}\n",
    "degree = [[xd for xd in tf_degree.values()], [ad for ad in gn_degree.values()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.xticks(np.arange(0, 300, step=10), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Degree\",fontsize=20)\n",
    "plt.ylabel(\"Nodes (%)\",fontsize=20)\n",
    "plt.title(\"Histogram of degrees\", fontsize=20)\n",
    "\n",
    "deg_tf = degree[0]\n",
    "deg_gn = degree[1]\n",
    "hist_data = np.array([deg_tf, deg_gn], dtype=object)\n",
    "plt.hist(hist_data, color = ['mediumorchid', 'coral'], \n",
    "         edgecolor = 'black', label = [\"TF's nodes\", \"gene's nodes\"], density = True, bins = 50)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genes have a lower degree ($\\lt 15$) than transcription factors which can have a very high degree. This seems logical as the transcription factors can regulate several genes and can be regulated by other TF but genes that doesn't code for a TF are at the end of this regultation chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2 - Clusturing coefficient\n",
    "The clusturing coefficient of a node reflects the inter-connectedness of its neighbors. It's defined as :\n",
    "$$C(v_i) = \\frac{|neighbors\\_of\\_v_i\\_connected|}{|pair\\_of\\_distinct\\_neighbors\\_of\\_v_i|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(G, V = G.nodes()) :\n",
    "    # Computes the clusturing coefficient of the nodes in V\n",
    "    # G : networkx graph\n",
    "    # v : list of nodes of G\n",
    "    # returned value : dict {node : int}, the int beeing the clusturing coefficiten of the node\n",
    "    \n",
    "    clust = {v : 0 for v in V} # returned value : clusturing coefficiten of each node\n",
    "    \n",
    "    for v in V : \n",
    "        e_neigh = 0 # number of edges between the neighbors of v\n",
    "        deg = len(G[v]) # degree of v\n",
    "        if deg > 1 :\n",
    "            for neigh1 in G[v] :\n",
    "                for neigh2 in G[neigh1] :\n",
    "                    if neigh2 in G[v] :\n",
    "                        e_neigh += 1\n",
    "        \n",
    "            clust[v] = e_neigh/(deg*(deg-1))\n",
    "        \n",
    "    return clust    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of each loop, n beeing the number of nodes in V :\n",
    "\n",
    "- `for v in V :` $n$\n",
    "    - `for neigh1 in G[v] :` $n-1$\n",
    "        - `for neigh2 in G[neigh1] :` $n-1$\n",
    "    \n",
    "So the time complexity of `clustering` is $O(n^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_clust = clustering(G, tf_nodes)\n",
    "gn_clust = clustering(G, gn_nodes)\n",
    "G_clust = clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mean_clust = np.mean([x for x in tf_clust.values()])\n",
    "gn_mean_clust = np.mean([x for x in gn_clust.values()])\n",
    "G_mean_clust = np.mean([x for x in G_clust.values()])\n",
    "tf_sd_clust = np.std([x for x in tf_clust.values()])\n",
    "gn_sd_clust = np.std([x for x in gn_clust.values()])\n",
    "G_sd_clust = np.std([x for x in G_clust.values()])\n",
    "\n",
    "\n",
    "print(\"Mean clustering coefficient TF :\", tf_mean_clust, \"sd :\", tf_sd_clust)\n",
    "print(\"Mean clustering coefficient genes :\", gn_mean_clust, \"sd :\", gn_sd_clust)\n",
    "print(\"Mean clustering coefficient total graph :\", G_mean_clust, \"sd :\", G_sd_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those coefficient calculated have a pretty high standard deviation, we cannot conclude on a general tendency in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.3 - Overlap\n",
    "\n",
    "Fraction of common neighbors of a connected pair.\n",
    "\n",
    "$$O(<v_i, v_j>) = \\frac{n_{v_i,v_j}}{(k_{v_i}-1) + (k_{v_j}-1) - n_{v_i, v_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(G, V = G.nodes()):\n",
    "    # Computes the overlap of the nodes in V, the average overlap on V and it's standard deviation\n",
    "    # G : networkx graph\n",
    "    # v : list of nodes of G\n",
    "    # returned value : (mean, sd, { vi :{vj : overlap} }), \n",
    "    # with vi and vj two nodes of G and overlap, mean and sd floats\n",
    "    \n",
    "    ovlp = {}\n",
    "    O_val = []\n",
    "    \n",
    "    # computation of overlap on G's neighbor nodes\n",
    "    for vi in V :\n",
    "        for vj in G[vi] :\n",
    "           \n",
    "            if vj in ovlp and vi in ovlp[vj] : # if the overlap has already been computed for (vj, vi)\n",
    "                O = ovlp[vj][vi]\n",
    "\n",
    "            elif G.degree(vi) <= 1 or G.degree(vj) <= 1 : # no possible common neighbor \n",
    "                O = 0\n",
    "\n",
    "            else :\n",
    "\n",
    "                count = 0 # counter for common neighbors of vi and vj\n",
    "\n",
    "                #searching for common neighbors\n",
    "                for nei in G[vi]:\n",
    "                    if nei in G[vj]:\n",
    "                        count+=1\n",
    "\n",
    "                if (G.degree(vi)-1+G.degree(vj)-1-count) == 0:\n",
    "                    O = 1\n",
    "                elif count == 0:\n",
    "                    O = 0\n",
    "                else :\n",
    "                    O =  count/(G.degree(vi)-1+G.degree(vj)-1-count)\n",
    "\n",
    "            O_val.append(O)\n",
    "            if vi not in ovlp :\n",
    "                ovlp[vi] = {}\n",
    "            ovlp[vi][vj] = O\n",
    "            \n",
    "\n",
    "                \n",
    "    mean_ovlp = np.mean(O_val)\n",
    "    sd_ovlp = np.std(O_val)\n",
    "    return (mean_ovlp, sd_ovlp, ovlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of each loop, n beeing the number of nodes in V :\n",
    "\n",
    "- `for vi in V :` $n$\n",
    "    - `for vj in G[vi] :` $n-1$\n",
    "        - `for nei in G[vi] :` $n-1$\n",
    "    \n",
    "So the time complexity of `overlap` is $O(n^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ovG = overlap(G)\n",
    "ovTF = overlap(G, tf_nodes)\n",
    "ovGN = overlap(G, gn_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean overlap for G :', ovG[0])\n",
    "print('Sd :', ovG[1])\n",
    "print('\\nMean overlap for TF :', ovTF[0])\n",
    "print('Sd :', ovTF[1])\n",
    "print('\\nMean overlap for genes :', ovGN[0])\n",
    "print('Sd :', ovGN[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overlap is a bit higher for transcription factors than for other genes, but the overlap values are very diverse for any type of node in the graph as we can see with the high standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.4 - Closeness centrality\n",
    "\n",
    "Inverse of the average distance of a node $v_i$ to the other nodes of the graph.\n",
    "\n",
    "$$d_{avg}(v_i) = \\frac{\\sum{dist(v_i, v_j)}}{|V| - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCentG = nx.closeness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCentG_val = [CCentG[n] for n in G.nodes()]\n",
    "CCentTF_val = [CCentG[x] for x in tf_nodes]\n",
    "CCentGN_val = [CCentG[a] for a in gn_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Mean closeness centrality for G :', np.mean(CCentG_val))\n",
    "print('Sd :', np.std(CCentG_val))\n",
    "print('\\nMean closeness centrality for TF :', np.mean(CCentTF_val))\n",
    "print('Sd :', np.std(CCentTF_val))\n",
    "print('\\nMean closeness centrality for genes :', np.mean(CCentGN_val))\n",
    "print('Sd :', np.std(CCentGN_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closeness centrality seems to be quite uniform on all the nodes of the graph. There isn't much difference between nodes that represent transcription factors and nodes that represent other genes, also, the standard deviation to the mean is fairly low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.5 - Betweeness centrality\n",
    "\n",
    "Fraction of shortest paths that passes through a node $v_i$.\n",
    "\n",
    "$$b(v_i) = \\sum_{v_s\\neq v_i \\neq v_t}{\\frac{\\sigma_{v_s, v_t}(v_i)}{\\sigma_{v_s,v_t}}}$$\n",
    "\n",
    "With :\n",
    "- $\\sigma_{v_s, v_t}(v_i)$ : number of shortest paths between $v_s$ and $v_t$ passing through $v_i$ ;\n",
    "- $\\sigma_{v_s,v_t}$ : number of shortest paths between $v_s$ and $v_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCentG = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCentG_val = [BCentG[n] for n in G.nodes()]\n",
    "BCentTF_val = [BCentG[x] for x in tf_nodes]\n",
    "BCentGN_val = [BCentG[a] for a in gn_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean betweeness centrality for G :', np.mean(BCentG_val))\n",
    "print('Sd :', np.std(BCentG_val))\n",
    "print('\\nMean betweeness centrality for TF :', np.mean(BCentTF_val))\n",
    "print('Sd :', np.std(BCentTF_val))\n",
    "print('\\nMean betweeness centrality for genes :', np.mean(BCentGN_val))\n",
    "print('Sd :', np.std(BCentGN_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a clearer difference between the set of nodes that represents transcription factors and the one representing other genes : the mean value of betweeness centrality for the transcription factors is clearly higher than the one of other genes. It's not very surprising as genes that are not transcription factors are at the end of the transcription regulation cascade so those can't be very central in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.6 - Density of the graph\n",
    "\n",
    "The density of our graph is the ratio of the actual number of edges in our graph compared to the maximal possible number of edges.\n",
    "\n",
    "$$\\rho = \\frac{2|E|}{|V|(|V|-1)}$$\n",
    "\n",
    "The complexity of computation is $O(1)$ as it consist of a unique operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityG = 2*len(G.edges)/(len(G.nodes)*(len(G.nodes)-1))\n",
    "nxdensity = nx.density(G)\n",
    "densityG == nxdensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect the density of our graph is low which is often the case for real networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.7 - Rich-club coefficient\n",
    "\n",
    "Represents how well connected are the well connected nodes between themselves.\n",
    "\n",
    "$$\\phi(k)=\\frac{2|E_{> k}|}{|V_{> k}|\\times(|V_{> k}|-1)}$$\n",
    "\n",
    "With :\n",
    "- $|V_{> k}|$ : nodes $v\\in V$ with $deg(v)> k$ ;\n",
    "- $|E_{> k}|$ : edges between nodes in $|V_{> k}|$ ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rich_club_coefficient(G, V = G.nodes, k = None) :\n",
    "    # Computes the rich club coefficient for the graph G for the degree k if passed, for all degrees if not\n",
    "    # G : networkx graph\n",
    "    # v : list of nodes of G\n",
    "    # k : int\n",
    "    # returned value : rich club coefficient as a float if argument k passed\n",
    "    # else : {k : rich club coefficient}, k as int and the coefficient as float\n",
    "    \n",
    "    deg = {v : len(G[v]) for v in V} \n",
    "    \n",
    "    if k != None : # if k passed\n",
    "        Vk = []\n",
    "        for v in V :\n",
    "            if deg[v] > k :\n",
    "                Vk.append(v)\n",
    "        Gk = G.subgraph(Vk)\n",
    "        Ek = Gk.edges\n",
    "\n",
    "        if len(Vk)>1 :\n",
    "            RC = 2*len(Ek)/(len(Vk)*(len(Vk)-1))\n",
    "        else :\n",
    "            RC = 0\n",
    "        return RC\n",
    "    \n",
    "    RC = {x : None for x in range(max(deg.values()))}\n",
    "\n",
    "    for x in RC :\n",
    "        RC[x] = rich_club_coefficient(G, V, x)\n",
    "    \n",
    "    return RC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of each loop, n beeing the number of nodes in V :\n",
    "\n",
    "- `deg = {v : len(G[v]) for v in V}` $n$\n",
    "- ` if k != None : # if k passed` \n",
    "    - `for v in V :` $n$\n",
    "\n",
    "So the time complexity of `rich_club_coefficient` with an argument `k` passed is $O(2n)$.\n",
    "\n",
    "- `deg = {v : len(G[v]) for v in V}` $n$\n",
    "- `RC = {x : None for x in range(max(deg.values()))}` $n-1$\n",
    "- `for x in RC :` $n-1$\n",
    "    - `RC[x] = rich_club_coefficient(G, V, x)` : $O(2n)$\n",
    "    \n",
    "So the time complexity of `rich_club_coefficient` without an argument `k` passed is $O(2n²)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RC = rich_club_coefficient(G)\n",
    "# RC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric can also be normalized by dividing it by the rich club coefficient of a maximally randomized network with the same degree distribution as the one we are studying :\n",
    "$${\\displaystyle \\rho _{rand}(k)={\\frac {\\phi (k)}{\\phi _{rand}(k)}}}$$\n",
    "\n",
    "With this normalized coefficient we can see if a Rich-club effect is really present or is the value of the coefficient is the same for a random network.\n",
    "\n",
    "As we need to construct a randomized network, the complexity increases compared to the one calculated above.\n",
    "\n",
    "Here we use the function implemented in the package `NetworkX` to compute the normalized coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCnorm = nx.rich_club_coefficient(G)\n",
    "# RCnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(RC.items()) # sorted by key, return a list of tuples\n",
    "listsnorm = sorted(RCnorm.items())\n",
    "\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "v, w = zip(*listsnorm)\n",
    "\n",
    "plt.plot(x, y, label = 'Rich-club coefficient')\n",
    "plt.plot(v, w, color = 'mediumorchid', label = 'Normalized Rich-club coefficient')\n",
    "plt.xlabel(\"Degree k\",fontsize=10)\n",
    "plt.ylabel(\"Rich Club Coefficient\",fontsize=10)\n",
    "plt.title('Rich-club coefficient for graph G')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we represent the results computed with the two different methods, we can see that for low values of `k`, the results are differents but, as `k` increases, the graphs of the two methods converges. As this coefficient is more relevant for high values of `k`, we could have only computed the non-normalized values that are less complex to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 -  Implement and apply the k-shell decomposition algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first try to make a k-shell with a function `k_shell` directly implemented in the `networkx.algorithms.core` library, for different $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(nx.k_shell(G, k=2), node_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.draw(nx.k_shell(G, k=4), node_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(nx.k_shell(G, k=6), node_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(nx.k_shell(G, k=7), node_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k > 7$ there is no more node left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet we try to implement ourself the `k_shell` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kshell (G, k) :\n",
    "    degrees = []\n",
    "    GG = G.copy()\n",
    "    \n",
    "    ik = 1\n",
    "    for ik in range(k):\n",
    "        if len(GG.nodes()) == 0 : # if there are no more nodes in the graph, the loop is exited\n",
    "            break\n",
    "        done = False\n",
    "        while not done :\n",
    "            rm = []\n",
    "            \n",
    "            # nodes of degree <= ik are removed\n",
    "            for n in GG.nodes():\n",
    "                if GG.degree(n)<=ik:\n",
    "                    rm.append(n)       \n",
    "            for m in rm :\n",
    "                GG.remove_node(m)\n",
    "            done = True\n",
    "            \n",
    "            # if nothing was removed, there is no need to recheck the degrees\n",
    "            # the while loop is exited\n",
    "            if len(rm) == 0 : \n",
    "                break\n",
    "            \n",
    "            # if there is at least one node of degree <= ik after we've removed nodes\n",
    "            # the while loop reiterates \n",
    "            for n in GG.nodes() :\n",
    "                if GG.degree(n)<=ik:\n",
    "                    done = False\n",
    "                    break\n",
    "            \n",
    "    return GG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of each loop, $n$ beeing the number of nodes in the entry graph `G` :\n",
    "\n",
    "- `for ik in range(k) :` $a\\leq k$ because the loop is exited if no nodes remain in the G\n",
    "    - `while not done :` $b$\n",
    "        - `for n in GG.nodes() :` $n$\n",
    "        - `for m in rm :` $c$, with `c = len(rm)`\n",
    "        - `for n in GG.nodes() :` $n-c$\n",
    "    \n",
    "As the `for` loop is exited if no nodes remain in the graph and in the worst case where each `while` loop removes nodes one by one, we have $a+b=n$. So the time complexity of `my_kshell` is $O(a\\times(n-a)\\times(n+c+(n-c))) = O(2n\\times a(n-a))$. As $a(n-a)$ is maximised for $a = \\frac{n}{2}$, in the worst case senario, the time complexity of `my_kshell` is $O(\\frac{n^3}{2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_G = my_kshell(G, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(new_G, node_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tf_nodes = []\n",
    "new_gn_nodes = []\n",
    "\n",
    "for x in new_G.nodes :\n",
    "    if x in tf_nodes :\n",
    "        new_tf_nodes.append(x)\n",
    "    elif x in gn_nodes :\n",
    "        new_gn_nodes.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = nx.spring_layout(new_G, iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On rend le graphe plus lisible \n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(new_G, new_pos, nodelist=new_gn_nodes, node_color=\"r\", node_size= 18, label = \"gene's nodes\")\n",
    "nx.draw_networkx_nodes(new_G, new_pos, nodelist=new_tf_nodes, node_color=\"b\", node_size= 18, label = \"TGF's nodes\")\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(new_G, node_size = 18, pos = new_pos, width = 0.4, edge_color = 'DarkSlateGray')\n",
    "\n",
    "plt.legend()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could study the components of this k-core by first associating the nodes in it with the genes they represent and there biological function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def function_in_G(G) :\n",
    "    name_G = [net4_gene_ids['Name'][net4_gene_ids['ID']==g].values[0] for g in G.nodes] # names of the genes in G\n",
    "\n",
    "    GO_G = [] # GO of the genes in G\n",
    "    for name in name_G :\n",
    "        for go in go_slim_mappingtab[5][go_slim_mappingtab[0] == name] : # GO associated to the gene\n",
    "                    if isinstance(go, str) : # excudes NaN\n",
    "                        GO_G.append(go)\n",
    "\n",
    "    func_G = [go_slim_mappingtab[4][go_slim_mappingtab[5] == go].values[0] for go in GO_G] # functions in new_G\n",
    "    \n",
    "    return func_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_count(G) :\n",
    "    func_G = function_in_G(G)\n",
    "    \n",
    "    count_func_G = {} # count of each function in G\n",
    "    for f in func_G :\n",
    "        if f not in count_func_G :\n",
    "            count_func_G[f] = 1\n",
    "        else :\n",
    "            count_func_G[f] += 1\n",
    "    \n",
    "    return count_func_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func_G = func_count(G) # count of each function in G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func_newG = func_count(new_G) # count of each function in new_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of count of functions in newG compared to G\n",
    "func_newG_G = {f : count_func_newG[f]/count_func_G[f] for f in count_func_newG}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Community detection\n",
    "\n",
    "#### 1.2.1 - You can choose between the Girvan Newman method and the Louvain algorithm tofind communities in the graph. Describe both algorithms, and their time complexities (explain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Girvan Newman method :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express Girvan-Newman algorithm in the following procedure:\n",
    "- Calculate edge betweenness for every edge in the graph ; \n",
    "- Remove the edge with highest edge betweenness ;\n",
    "- Calculate edge betweenness for remaining edges ;\n",
    "- Repeat steps 2–4 until all edges are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest and fastest form - worst-case time complexity is $O(|E|^{2}|V|$) for a graph $G = < V, E >$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Louvain algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method consists of two phases :\n",
    "- The first step is to look for \"small\" communities by optimizing modularity in a local way, where the modularity quantifies the quality of an assignment of nodes to communities. \n",
    "- The second step consist of an aggregation of nodes from the same community and to build a new network whose nodes are the communities. \n",
    "\n",
    "These steps are repeated iteratively until a maximum of modularity is attained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The algorithm is:\n",
    "\n",
    "- Start with each node being a singleton cluster: \n",
    "- Consider nodes in random order.\n",
    "- Iterate as long as cluster membership changes \n",
    "     - for each node : remove it from its current cluster and add it to the cluster with the highest modularity gain \n",
    "- aggregate the resulting clustering to a new graph and continue on next level (step 1), as long as modularity improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time complexity of the Louvain algorithm is $O(|V| \\log(|V|))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 - Which algorithm did you choose, why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the Louvain method because this method has the best time complexity. Moreover we will use the the module *community* to implement a community detection. By default this module uses the Louvain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_best_partition = community_louvain.best_partition(G, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity = community_louvain.modularity(louvain_best_partition, G, weight='weight')\n",
    "print(\"The modularity Q of G's best partition is {}\".format(modularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = nx.spring_layout(G, k=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "size = float(len(set(louvain_best_partition.values())))\n",
    "count = 0\n",
    "\n",
    "cmap = cm.get_cmap('viridis', max(louvain_best_partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, louvain_best_partition.keys(), node_size=18, cmap=cmap, \n",
    "                       node_color=list(louvain_best_partition.values()))\n",
    "nx.draw_networkx_edges(G, node_size = 18, pos = pos, width = 0.5, \n",
    "                       edge_color = 'DarkSlateGray', alpha= 0.6)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dendrogram is a tree and each level is a partition of the graph nodes.  Level 0 is the first partition, whichcontains the smallest communities, and the best is len(dendrogram) - 1.  The higher the level is, the bigger are the communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = community_louvain.generate_dendrogram(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\" There are \", len(dendrogram) - 1 , \"levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of dictionaries in which keys are the nodes and the values are the set it belongs to, and the dictionnaies represent the diffrent levels of partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dict with the community ID (numbers 1, 2, 3, ...n) as keys \n",
    "#and the genes and factors that belong to it as values.\n",
    "\n",
    "communities_classes = {}\n",
    "\n",
    "for comm in louvain_best_partition.values():\n",
    "    communities_classes[comm] = communities_classes.get(comm, [])\n",
    "    \n",
    "for gene in louvain_best_partition.keys():\n",
    "    communities_classes[louvain_best_partition[gene]].append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "communities_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from collections import Counter\n",
    "louvain_best_partition_ordered = {k : v for k, v in sorted(louvain_best_partition.items(), key = lambda item: item[1])}\n",
    "comm_size = Counter(louvain_best_partition_ordered.values())'''\n",
    "comm_size = {c : len(communities_classes[c]) for c in communities_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comm_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.xticks(np.arange(0, 300, step=10), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Communities ID \",fontsize=20)\n",
    "plt.ylabel(\"Number of nodes per community\",fontsize=20, )\n",
    "plt.title(\"Plot of the communities fund with the best partition detection\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.plot([c for c in sorted(comm_size.keys())], [n for n in comm_size.values()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(communities_classes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the community 0 has the biggest number of nodes. It means that in this community, there is a bigger interaction between genes dans factors than in the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 - For the the Girvan Newman method, the user should select one of the output partitions, explain the criterion that could be used to make this choice, and its complexity.\n",
    "\n",
    "We could compute the modularity for each output partition and select the one with the maximal modularity value. We can compute the modularity with :\n",
    "\n",
    "$${\\displaystyle Q={\\frac {1}{2|E|}}\\sum _{vw}\\left[A_{vw}-{\\frac {k_{v}k_{w}}{2|E|}}\\right]\\delta (c_{v},c_{w})}$$\n",
    "\n",
    "With :\n",
    "- $2|E|$ : the number of edges in the graph ;\n",
    "- $A_{vw}$ : number of edges between nodes $v$ and $w$ ;\n",
    "- $k_{v}, k_{w}$ : degrees of nodes $v$ and $w$ ;\n",
    "- $\\frac {k_{v}k_{w}}{2|E|}$ : expeted number of edges between two nodes of degree $k_{v}$ and $k_{w}$ ;\n",
    "- $\\delta (c_{v},c_{w}) = 1$ if nodes $v$ and $w$ are in the same community, else $\\delta (c_{v},c_{w}) = 0$.\n",
    "\n",
    "We can find the source code of the modularity function in `NetworkX` [here](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html?highlight=modularity) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def modularity(G, communities, weight=\"weight\"):\n",
    "    \n",
    "    directed = G.is_directed()\n",
    "    if directed:\n",
    "        out_degree = dict(G.out_degree(weight=weight))\n",
    "        in_degree = dict(G.in_degree(weight=weight))\n",
    "        m = sum(out_degree.values())\n",
    "        norm = 1 / m ** 2\n",
    "    else:\n",
    "        out_degree = in_degree = dict(G.degree(weight=weight))\n",
    "        deg_sum = sum(out_degree.values())\n",
    "        m = deg_sum / 2\n",
    "        norm = 1 / deg_sum ** 2\n",
    "\n",
    "    def community_contribution(community):\n",
    "        comm = set(community)\n",
    "        L_c = sum(wt for u, v, wt in G.edges(comm, data=weight, default=1) if v in comm)\n",
    "\n",
    "        out_degree_sum = sum(out_degree[u] for u in comm)\n",
    "        in_degree_sum = sum(in_degree[u] for u in comm) if directed else out_degree_sum\n",
    "\n",
    "        return L_c / m - out_degree_sum * in_degree_sum * norm\n",
    "\n",
    "    return sum(map(community_contribution, communities))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an undirected graph $G = < V, E >$ and $i$ communities :\n",
    "\n",
    "- `def community_contribution(community):`\n",
    "    - `L_c = sum(wt for u, v, wt in G.edges(comm, data=weight, default=1) if v in comm)` $|E_C|$\n",
    "    - `out_degree_sum = sum(out_degree[u] for u in comm)` $|V_C|$\n",
    "\n",
    "So the time complexity of `Community_contribution` is $O(|E_C|+|V_C|)$.\n",
    "\n",
    "- `return sum(map(community_contribution, communities))` $\\sum_i (|E_{C_i}|+|V_{C_i}|) = |E|+|V|$\n",
    "    \n",
    "So the time complexity of `modularity` is $O(|E|+|V|)$, so if we have $|V|=n$ we can say that `modularity` is $O(\\frac{n(n-1)}{2} + n) = O(\\frac{n(n+1)}{2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as we want the best partition in all the partitions given by the Girvan Newman method, we have to compute the modularity for $|E|$ partitions. The thime complexity of our choice is then $O(\\frac{n(n+1)}{2}\\times \\frac{n(n-1)}{2})$ which is $O(\\frac{n^4}{4})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 - Study the GO composition of each community.  To do this you can produce a countingmatrix $M$,  such  that $M_{i,j}$ is  the  number  of  genes  from  community $j$ that  have  GO annotation $i^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of all the GO in the file go_slim_mappingtab\n",
    "GO_rep = [go for go in go_slim_mappingtab[5]]\n",
    "GO_unsorted = []\n",
    "for go in GO_rep :\n",
    "    if isinstance(go, str) and go not in GO_unsorted :\n",
    "        GO_unsorted.append(go)\n",
    "GO = sorted(GO_unsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_comm = {go : [0]*len(comm_size.keys()) for go in GO} # dict GO : [n° of occurence in each community]\n",
    "\n",
    "for comm in communities_classes :\n",
    "    for gene in communities_classes[comm] : \n",
    "        \n",
    "        # for each gene in each community\n",
    "        # count the number of occurence of each GO\n",
    "        \n",
    "        name = net4_gene_ids['Name'][net4_gene_ids['ID'] == gene].values[0] # name of the gene\n",
    "        for go in go_slim_mappingtab[5][go_slim_mappingtab[0] == name] : # GO associated to the gene\n",
    "            if isinstance(go, str) : # excudes NaN\n",
    "                GO_comm[go][comm] += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M0 = pd.DataFrame(data = GO_comm)\n",
    "M0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 - Is there a relationship between graph communities and particular cell functions ?\n",
    "\n",
    "Firstly we are going to exclude the GO having too many counts as they denote too general features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# suppression of GO with too many counts\n",
    "\n",
    "GO_sum = [sum(M0[go])for go in M0.columns] # Total number of nodes of each GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphical representation of GO_sum : boxplot representation\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.ylabel(\"Number of nodes\",fontsize=20)\n",
    "plt.title(\"Total number of nodes of each GO in the network\", fontsize=20)\n",
    "\n",
    "plt.boxplot(GO_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have several outliers that have a high number of nodes corresponding to their GO annotation. We will now remove the corresponding GO annotations from our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IQR = np.percentile(GO_sum, 75) - np.percentile(GO_sum, 25) # interquartil range\n",
    "maxGOaccepted = np.percentile(GO_sum, 75) + 1.5*IQR # count threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_del = [go for go in M0.columns if sum(M0[go])>maxGOaccepted] # GO with counts over the threshold\n",
    "M1 = M0.copy() \n",
    "for go in GO_del :\n",
    "    M1.pop(go)\n",
    "# M1 is the matrix M0 with columns corresponding to GO with counts over the threshold removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M1.shape < M0.shape # test that columns have been deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for each GO if it's equaly represented in each community. To do so we will do a chi2 test for each G0 we have in the matrix. The null hypothesis $H_0$ of our test is the following :\n",
    "\n",
    "$H_0$ : \"The proportion of G0 is the same in all the communitites\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadratic distance between observed values and theoretical values calculated under H_0\n",
    "E = {go : 0 for go in M.columns}\n",
    "for go in M.columns :\n",
    "    i = range(len(communities_classes)) # communitites\n",
    "    ki = M[go].values # proportion of go in each community i\n",
    "    if sum(ki) != 0 :\n",
    "        ni = [comm_size[x] for x in i] # size of each community i\n",
    "        p_star = sum(ki)/sum(ni) # proportion of go in all the community under H_0\n",
    "        ki_star = [p_star*n for n in ni]\n",
    "        non_ki_star = [(1-p_star)*n for n in ni]\n",
    "        E[go] = sum( (ki - ki_star)**2/ki_star + ((ni-ki) - non_ki_star)**2/non_ki_star )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# computation of the p-value of the chi2 test for each GO\n",
    "\n",
    "GO_p = {go : None for go in M.columns} # p-values\n",
    "\n",
    "for go in M.columns :\n",
    "    k = [m for m in M[go].values] # proportion of genes with the annotation go in each community\n",
    "    \n",
    "    if sum(k) == 0 : # if no genes has the annotation go\n",
    "        GO_p[go] = 1\n",
    "        \n",
    "    else :\n",
    "        n = [comm_size[comm] for comm in sorted(comm_size.keys())] # size of each community\n",
    "        nonk = [n[i] - k[i] for i in range(len(communities_classes))] # not k\n",
    "        cont = [k, nonk] # contingency table\n",
    "        GO_p[go] = st.chi2_contingency(cont)[1] # p-value for go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001 # risk of rejecting H_0 while H_0 true of 0.01%\n",
    "\n",
    "# GO that aren't represented equaly throughout the communities with a risk alpha \n",
    "H0_rejected = [go for go in M.columns if GO_p[go] < alpha] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M[H0_rejected] # Corresponding columns in M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell function corresponding to the GO annotation \n",
    "cell_func = [go_slim_mappingtab[4][go_slim_mappingtab[5] == go].values[0] for go in H0_rejected] \n",
    "cell_func_df = pd.DataFrame(data = np.asarray(M[H0_rejected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# displaing the matrix of the rejected columns with the cell function names associated\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "cell_func_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk $\\alpha$ taken is very low and we still have a lot of cell functions that are particularly more represented in some communities so we can say that there is a relationship between graph communities and particular cell functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Around the Traveling Salesman Problem\n",
    "### 2.1 - Exact solution\n",
    "#### 2.1.1 - Number of Hamiltonian paths in a complete graph\n",
    "\n",
    "In a complete graph, every node is adjacent to every other node. Let $G = (V, E)$ a complete graph with $|V| = n$. Let's define an Hamiltonian path $(x_0, x_1, x_2,...,x_n)$ in $G$. There are $n$ possible starting nodes $x_0$. Then, as one of the $n$ nodes has already been visited, only $n-1$ options are left for $x_1$, $n-2$ for $x_2$ and so on until there is only one node left unvisited : $x_n$. Thus, there are $n\\times n-1 \\times n-2 \\times ... \\times 1 = n!$ distinct Hamiltonian paths in a complete graph of size $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Test graphs creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a complete graph to test our functions\n",
    "Gc = nx.Graph()\n",
    "Gc = nx.complete_graph(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "list(Gc.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(G) : \n",
    "    # Modifies G to add a random int weight in ]0,10]\n",
    "    # G : networkX graph\n",
    "    for e in G.edges():\n",
    "        G[e[0]][e[1]]['weight'] = random.randrange(0,10)+1\n",
    "        #print(e, Gc[e[0]][e[1]]['weight']) \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_weight(G):\n",
    "    # Draws the graph G where each edge's width is proportional to its weight\n",
    "    # G : weighted networkX graph\n",
    "    \n",
    "    edge_width = [d['weight'] for (u, v, d) in G.edges(data=True)] # weight of the edges of G\n",
    "\n",
    "    pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G, pos)\n",
    "\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_width)\n",
    "\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_weight(Gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_weight(Gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a different graph to test our functions\n",
    "Gtest = nx.petersen_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_weight(Gtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_weight(Gtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 - Enumeration of Hamiltonian paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(G, si, path, weight, path_weight) :\n",
    "    # Step to compute all possible paths that go as deep as possible from si without any nodes already in path,\n",
    "    # also computes corresponding weight of these paths\n",
    "    # G : weighted networkX graph\n",
    "    # si : node of G, active node\n",
    "    # path : list of nodes of G, active path\n",
    "    # weight : int, weight of the active path\n",
    "    # path_weight : list of tuples (path, weight), \n",
    "    # each path is represented as a list of nodes, its total weight as an int\n",
    "    \n",
    "    if len(path)>0 : # if si is not the fist node of the path\n",
    "        sp = path[len(path)-1] # last node in path\n",
    "        si_w = G[sp][si]['weight'] # weight of the edge between sp and si\n",
    "        weight = weight + si_w # weight update\n",
    "        \n",
    "    path = path + [si] # si is added to the path\n",
    "    \n",
    "    # si_succ is filled with the successors of si that are not already in path\n",
    "    si_succ = [] \n",
    "    for sj in G[si] :\n",
    "        if sj not in path :\n",
    "            si_succ.append(sj)\n",
    "    \n",
    "    # next_step is called for each succesor sj\n",
    "    for sj in si_succ :\n",
    "        next_step(G, sj, path, weight, path_weight)\n",
    "    \n",
    "    # when each branch has been visited as deep as possible, the list of pathecessor is added to path\n",
    "    if len(si_succ) == 0 :\n",
    "        path_weight.append((path, weight))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamiltonian_path(G, s0) :\n",
    "    # Computes the the possible Hamiltonian paths in G starting at node s0\n",
    "    # G : weighted networkX graph\n",
    "    # s0 : node in G\n",
    "    # returned value : list of tuples (path, weight)\n",
    "    # each path is represented as a list of nodes, its total weight as an int\n",
    "    \n",
    "    # computing all possible paths that go as deep as possible and start at s0 and their corresponding weight\n",
    "    \n",
    "    path = [] # active path\n",
    "    path_weight = [] # list of tuples (path, weight) to contain the paths and their weight\n",
    "    weight = 0 # weight of the active path\n",
    "    \n",
    "    # first step\n",
    "    # active node : s0\n",
    "    next_step(G, s0, path, weight, path_weight) \n",
    "    \n",
    "    \n",
    "    # Filtering path_w to keep only the Hamiltonian paths \n",
    "    \n",
    "    n = len(G.nodes()) # size of G\n",
    "    ham_path = [] \n",
    "    for tup in path_weight :\n",
    "        if len(tup[0]) >= n :\n",
    "            ham_path.append(tup)\n",
    "          \n",
    "    return ham_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Hamiltonian_path(Gtest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gc_path = Hamiltonian_path(Gc, 'a')\n",
    "Gc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_Gc_path = len(Gc_path) # number of Hamiltonian paths found starting at node 'a' in Gc\n",
    "n = len(Gc.nodes()) # size of Gc\n",
    "print(N_Gc_path == math.factorial(n-1)) # test N_Gc_path == (n-1)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed before, we can find $n!$ Hamiltonian paths in a complete graph of size $n$. Here we know the starting node $n_1$ so we don't have the $n$ options for $n_1$ but only one, so we end up with $\\frac{n!}{n} = (n-1)!$ Hamiltonian paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 - Shortest Hamiltonian path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(path_w) :\n",
    "    # returns the shortest(s) path(s) of path_w\n",
    "    # if there are several paths of the same minimal weight they will be all returned\n",
    "    # path_w : list of tuples (path, weight)\n",
    "    # each path is represented as a list of nodes, its total weight as an int\n",
    "    # returned value : tuples (path, weight) \n",
    "    \n",
    "    w_min = path_w[0][1] # minimal weight initialisation\n",
    "    path_min = [] # shortest path initialisation\n",
    "    \n",
    "    # searching for the lowest weight\n",
    "    for tup in path_w :\n",
    "        if tup[1] < w_min : # if a lower weight is found\n",
    "            w_min = tup[1] # minimal weight actualisation\n",
    "            path_min = [tup] # shortest path actualisation\n",
    "        elif tup[1] == w_min : # if there is a path of the same weight as path_min\n",
    "            path_min.append(tup) # this path is added to the list of shortest paths\n",
    "            \n",
    "    return path_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_ham_path(G, s0) :\n",
    "    # returns the shortest(s) Hamiltonian path(s) in G starting at node s0\n",
    "    # if there are several paths of the same minimal weight they will be all returned\n",
    "    # G : weighted networkX graph\n",
    "    # s0 : node in G\n",
    "    # returned value : list of tuples (path, weight)\n",
    "    # each path is represented as a list of nodes, its total weight as an int\n",
    "    \n",
    "    path_w = Hamiltonian_path(G, s0)\n",
    "    path_min = shortest_path(path_w)\n",
    "            \n",
    "    return path_min\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gc_min_path = shortest_ham_path(Gc, 'a')\n",
    "Gtest_min_path = shortest_ham_path(Gtest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gc_min_path', Gc_min_path)\n",
    "print('Gtest_min_path', Gtest_min_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the research of all Hamiltonian paths, `si_succ`, on which we loop, gets smaller as we go deeper. For `si = s0`, the maximum size of `si_succ` is $n$ (if we can have an edge that liks `si` to itself), then as we go a step deeper, the maximum size of `si_succ` is $n-1$, then $n-2$, and so on. The time complexity of the method `Hamiltonian_path` is $O(n!)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 - Traveling Salesman exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traveling_Salesman(G) :\n",
    "    # Returns all possible solutions to the traveling salesman problem for the graph G\n",
    "    # G : weighted networkX graph\n",
    "    # returned value : list of tuples (path, weight)\n",
    "    # each path is represented as a list of nodes, its total weight as an int\n",
    "    \n",
    "    # searching shortests Hamiltonian paths in G for each of its nodes as starting node\n",
    "    path_w = []\n",
    "    for s in G.nodes() :\n",
    "        path_w = path_w + Hamiltonian_path(G, s)\n",
    "    \n",
    "    # finding the shortest(s) path(s) in the Hamilton paths found\n",
    "    path_min = shortest_path(path_w)\n",
    "    \n",
    "    return path_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveling_Salesman(Gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveling_Salesman(Gtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - The Nearest Neighbor heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(G, s0) :\n",
    "    # Implementation of the nearest neighbour heuristic\n",
    "    # Parameters :\n",
    "    # G : weighted networkX graph\n",
    "    # s0 : node in G\n",
    "    # Return value : tuple\n",
    "    # list of nodes, shortest Hamiltonian path starting at s0\n",
    "    # int, total weight of the path\n",
    "    # list of the nodes left unvisited\n",
    "    \n",
    "    shortest_path = [] # returned path\n",
    "    spath_w = 0 # total weight of the path\n",
    "    n = G.number_of_nodes() \n",
    "    \n",
    "    si = s0 # active node, initialised as s0\n",
    "    shortest_path.append(si) # si is added to the path\n",
    "    \n",
    "    while len(shortest_path) < n : # while there are still unvisited nodes in G\n",
    "        # the neighbors of si that has not been visited yet are stored in succ\n",
    "        succ = [] \n",
    "        for sj in G[si] :\n",
    "            if sj not in shortest_path :\n",
    "                succ.append(sj)\n",
    "        \n",
    "        succ_weight = [(s, G[si][s]['weight']) for s in succ] # weight of the edge linking si to its neighbors\n",
    "        \n",
    "        if len(succ) == 0 : # If there are no successor\n",
    "            return (shortest_path, spath_w, G.nodes()-shortest_path)\n",
    "        \n",
    "        # fiding the edge with the minimal weight\n",
    "        min_w = succ_weight[0][1] # minimal weight initialisation\n",
    "        min_s = succ_weight[0][0] # corresponding successor node\n",
    "        \n",
    "        for tupNW in succ_weight :\n",
    "            if tupNW[1] < min_w : # if a lower weight is found\n",
    "                min_w = tupNW[1] # min_w actualisation\n",
    "                min_s = tupNW[0] # min_s actualisation\n",
    "       \n",
    "        shortest_path.append(min_s) # the closest successor is added to the path\n",
    "        spath_w = spath_w + min_w\n",
    "        si = min_s # the closest successor is now the active node\n",
    "    \n",
    "    return (shortest_path, spath_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(Gtest, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbour(Gc, 'f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of each loop :\n",
    "\n",
    "- `while len(shortest_path) < n :` $n$\n",
    "    - `for sj in G[si] :` $n$\n",
    "    - `for tupNW in succ_weight :` $n$\n",
    "    \n",
    "So the time complexity of `nearest_neighbour` is $O(n \\times (n+n)) = O(2n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gex = nx.Graph()\n",
    "Eex = [(0, 1, {'weight': 4}), (0, 4, {'weight': 4}), (0, 5, {'weight': 2}), (1, 2, {'weight': 8}), (1, 6, {'weight': 1}), (2, 3, {'weight': 5}), (2, 7, {'weight': 4}), (3, 4, {'weight': 8}), (3, 8, {'weight': 8}), (4, 9, {'weight': 10}), (5, 7, {'weight': 6}), (5, 8, {'weight': 8}), (6, 8, {'weight': 5}), (6, 9, {'weight': 2}), (7, 9, {'weight': 7})]\n",
    "Gex.add_edges_from(Eex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbour(Gex, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveling_Salesman(Gex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour(Gex, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it fails to find the shortest Hamiltonian path in some cases, for example, in `Gex`, with `9` as a starting node, this method does not create an Hamiltonian path. And for `8` as a starting node, it creates an Hamiltonian path but not the shortest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - The Minimum Spanning Tree heuristic (MST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 - Time complexity of Prim's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an **adjacency list** is used to represent the graph, using a  **Breadth First Search (BFS)**, all the vertices can be browsed in  $O(V + E)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use a **min heap** as a priority queue to store vertices not yet included in the MST and to get the minimum weight edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Heap is a special Tree-based data structure in which the tree is a complete binary tree. In a Min-Heap the key present at the root node must be minimum among the keys present at all of it’s children. The same property must be recursively true for all sub-trees in that Binary Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **min heap** opération has a complexity time of $O(\\log(V))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have a total time compelxity of :\n",
    "\\begin{align*}\n",
    "    & = O(V + E) \\times O(\\log(V)) \\\\\n",
    "    & = O((V + E)\\log(V)) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 - Prove that during each iteration of the while loop, the subgraph (W, F ) is a tree.\n",
    "\n",
    "##### Initialisation :\n",
    "For the first iteration, $W = W_0 = \\{x_0\\}$ and $F = F_0 = \\emptyset$. \n",
    "\n",
    "> Let $(x, y) \\in E$ the shortest edge such that $x \\in W$ and $y \\notin W$\n",
    "\n",
    "- $x_0$ is the only element that is in $W_0$ so $x = x_0$ ;\n",
    "\n",
    "- let $x_1$ the closest neighbor of $x_0$, $y = x_1$.\n",
    "\n",
    "> $W \\gets W \\cup y$\n",
    "\n",
    "> $F \\gets F \\cup (x, y)$\n",
    "\n",
    "We now have $W = W_1 = \\{x_0, x_1\\}$ and $F = F_1 = \\{(x_0, x_1)\\}$.\n",
    "\n",
    "$(W, F) = (W_1, F_1)$ consist of only two nodes linked by one edge so it's an acyclic and connected graph, we can say that we have a tree at the end of this first iteration.\n",
    "\n",
    "##### Heredity :\n",
    "Let's suppose that at the end of the $n^{th}$ iteration we have a tree $(W, F) = (W_n, F_n)$ with $W_n = \\{ x_0, x_1, ..., x_n\\}$.\n",
    "\n",
    "> Let $(x, y) \\in E$ the shortest edge such that $x \\in W$ and $y \\notin W$\n",
    "\n",
    "- let $(x_i, x_{n+1}) \\in E$ the shortest edge such that $x_i \\in W_n$ and $x_{n+1} \\notin W_n$ ;\n",
    "- we thus have $x = x_i$ and $y = x_{n+1}$.\n",
    "\n",
    "> $W \\gets W \\cup y$\n",
    "\n",
    "> $F \\gets F \\cup (x, y)$\n",
    "\n",
    "At the end of this $(n+1)^{th}$ iteration we have $(W, F) = (W_{n+1}, F_{n+1})$ with $W_{n+1} = \\{ x_0, x_1, ..., x_n, x_{n+1}\\}$ and $F_{n+1} = F_n \\cup \\{(x_i, x_{n+1})\\}$.\n",
    "\n",
    "As $(W_n, F_n)$ is a tree and that we created $(W_{n+1}, F_{n+1})$ by adding one node $x_{n+1}$ and connecting it to the tree $(W_n, F_n)$ with one edge $(x_i, x_{n+1})$, $(W_{n+1}, F_{n+1})$ is connected and is acyclic because we didn't add any edge between two nodes of $(W_n, F_n)$.\n",
    "(\n",
    "##### Conclusion :\n",
    "We have shown that :\n",
    "- at the end of the first iteration of the `while` loop $(W, F)$ ;\n",
    "- if we have a subgraph $(W, F) = (W_n, F_n)$ that is a tree as a precondition to any iteration, we end up with the subgraph $(W, F) = (W_{n+1}, F_{n+1})$ that is also a tree at the end of the iteration.\n",
    "\n",
    "We can thus say that, according the recurrence principle, the subgraph $(W, F)$ that we have when the exiting condition of the `while` loop $W \\neq V$ is met is a tree and that $(W, F) = (V, F)$. So the subgraph $(V, F)$ returned at the end of Prim's algorithm is a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 - Show that the result of Prim’s algorithm is a MST\n",
    "\n",
    "We know that, if $(V, F)$ if a tree, then $|F| = |V|-1$, therefore, there is a fixed number of edges in $F$. The only way to minimize the sum of the weights of the edges of a spanning tree is to minimize each weight as we can't minimize the number of edges. At each iteration of the while loop, Prim's algorithm selects the shortest edge possible to attatch a new node to the tree so we can say that it returns a minimum spanning tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 - Write a Python function that implements Prim’s algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prims_algo(G,start):\n",
    "    # Function that recives a graph G and a starting node start, and returns a Minimum Spannin Tree MST\n",
    "    # G : weighted NetworkX graph\n",
    "    # start : node of G\n",
    "    # returned value : \n",
    "    \n",
    "    N = len(G) - 1 \n",
    "    current = start # current node\n",
    "    visited = set() # We store all the visited nodes\n",
    "    pq = pqdict.PQDict() #from the module pqdict (priority queue dictionnary) :\n",
    "    MST = []\n",
    "\n",
    "    while len(MST) < N:\n",
    "        # filling pq with the edges linking current node to its neighbors not already visited\n",
    "        for node in G.neighbors(current):\n",
    "            if node not in visited and current not in visited:\n",
    "                if (current,node) not in pq and (node,current) not in pq:\n",
    "                    w = G[current][node]['weight']\n",
    "                    pq.additem((current,node), w)\n",
    "\n",
    "        visited.add(current)\n",
    "        edge_, weight_ = pq.popitem() # We get the last tuple of the priority queue with its weight (lower weight edge)\n",
    "        \n",
    "        # removing potential edges that goes to a visited node\n",
    "        while(edge_[1] in visited):\n",
    "            edge_, weight_ = pq.popitem()\n",
    "            \n",
    "        MST.append(edge_)\n",
    "        current = edge_[1]\n",
    "\n",
    "\n",
    "    return MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSTtest = prims_algo(Gc, 'a')\n",
    "MSTtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_edges = [n for n in Gc.edges() if n not in MSTtest]\n",
    "other_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(Gc)\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(Gc, pos, node_size=500)\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(Gc, pos, edgelist=other_edges, width=4, alpha=0.5, edge_color='b', style='dashed')\n",
    "nx.draw_networkx_edges(Gc, pos, edgelist=MSTtest, width=4)\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(Gc,pos,font_size=20,font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triangular inequality is the following inequality:\n",
    "\\begin{equation*}\n",
    "    \\forall x, y, z \\in V, w(x, z) \\leq w(x, y) + w(y, z),\n",
    "\\end{equation*}\n",
    "\n",
    "where $w(x, y)$ is the weight of edge $x → y$ (more direct paths are shorter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 - Assuming that the graph verifies the triangle inequality, show that the length of the Hamiltonian cycle obtained by visiting the MST is less than twice the length of the shortest Hamiltonian cycle of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the shortest Hamiltonian cycle called $H_s$ and remove an arbitrary edge. The résult is a spannig tree called $T$. So we can first write the following equality if we consider that all edge have a non-negative weight :\n",
    "\n",
    "\n",
    "> $length(T) \\leq length (H_s)$\n",
    "\n",
    "\n",
    "Now, let $P$ be the full path of the Minimum Spanning Tree (MST) denoted by $T_{MST}$. This full path will include repeated vertices. \n",
    "\n",
    "Example : {0, 1, 2, 1, 7, 1, 0, 3, 4, 5, 6, 4, 3, 0}\n",
    "\n",
    "So as you can imagine the full path $P$ traverses every edges of the $T_{MST}$ twice. So we have :\n",
    "\n",
    "\n",
    "> $length(P) = 2 \\times length (T_{min})$\n",
    "\n",
    "> $length(P) = 2 \\times length (T_{min}) \\leq length(T)$\n",
    "\n",
    "> $length(P) = 2 \\times length (T_{min}) \\leq length(T) \\leq length(H_s)$\n",
    "\n",
    "The problem is $P$ is not a proper cycle since come vertices may be visited more than once. So by using the **triangle inequality**, we delete a visit to every vertices from $P$ to obtain a proper Hamiltonian cycle denoted $H$ where the **weight does not increase**.\n",
    "\n",
    "In our example : {0, 1, 2, 7, 3, 4, 5, 6, 0}\n",
    "\n",
    "Then we get :\n",
    "\n",
    "\n",
    "> $length(H) \\leq length (P)$\n",
    "\n",
    "> $length(H) \\leq length (P) \\leq 2 \\times length (T_{min})$\n",
    "\n",
    "> $length(H) \\leq length (P) \\leq 2 \\times length (T_{min}) \\leq length(T) \\leq length(H_s)$\n",
    "\n",
    "\n",
    "So finaly we have :\n",
    "\n",
    "\n",
    "> $length(H) \\leq 2 \\times length (H_s)$\n",
    "\n",
    "\n",
    "\n",
    "QED."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
